---
title: "Homework 02"
author: "Qianhui Rong"
date: "Septemeber 16, 2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

\newcommand{\mat}[1]{\boldsymbol{#1}} 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\rv}[1]{\underline{#1}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,dev="CairoPNG",fig.align = "center", 
                      fig.width = 5.656, fig.height = 4, global.par = TRUE)
pacman::p_load("arm","data.table","Cairo","faraway","foreign","ggplot2","knitr")
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
```

# Introduction 
In homework 2 you will fit many regression models.  You are welcome to explore beyond what the question is asking you.  

Please come see us we are here to help.

## Data analysis 

### Analysis of earnings and height data

The folder `earnings` has data from the Work, Family, and Well-Being Survey (Ross, 1990).
You can find the codebook at http://www.stat.columbia.edu/~gelman/arm/examples/earnings/wfwcodebook.txt
```{r}
gelman_dir <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
heights    <- read.dta (paste0(gelman_dir,"earnings/heights.dta"))
```

Pull out the data on earnings, sex, height, and weight.

1. In R, check the dataset and clean any unusually coded data.

```{r}
#Remove NA
na_data<-which(!complete.cases(heights)) #which don't have complete cases
heights_no_na<-heights[-na_data,] #remove na from original dataset
#Find out meaningless earning=0
zero_earn<-which(heights_no_na$earn==0)
heights_final<-heights_no_na[-zero_earn,]
#Check every variable's plot
par(mfrow=c(2,2))
hist(heights_final$earn)
hist(heights_final$height)
hist(heights_final$yearbn)
hist(heights_final$sex)
```

2. Fit a linear regression model predicting earnings from height. What transformation should you perform in order to interpret the intercept from this model as average earnings for people with average height?

To interpret the intercept from this model as average earnings for people with average height should use centering transformation.
```{r}
#Model1:center on mean
height_center<-heights_final$height-mean(heights_final$height)
Model1<-lm(earn~height_center,data = heights_final)
summary(Model1)
plot(resid(Model1))
abline(h=0)
ggplot(Model1)+aes(y=heights_final$earn,x=height_center)+geom_point(color="black")+geom_smooth()
```
In this model, 

$earn=23154.8+1262.3 height_{center}$

So if a person of average height, he or she will have an average earning of 23154.8. One unit from height's average leads to a difference of 1262.3 on earning.

3. Fit some regression models with the goal of predicting earnings from some
combination of sex, height, and weight. Be sure to try various transformations and interactions that might make sense. Choose your preferred model and justify.

```{r}
#center,z-score,log,interaction
#Model2:z-score transformation on height
zscore_height<-height_center/sd(heights_final$height)
Model2<-lm(earn~zscore_height,data = heights_final)
summary(Model2)
#Model3:log transformation on log(earn+1) 
Model3<-lm(log(earn)~height_center,data = heights_final)
summary(Model3)
plot(resid(Model3))
abline(h=0)
ggplot(data.frame(rs=rstudent(Model3),hat=hatvalues(Model3),cd=cooks.distance(Model3)))+geom_point( shape=21)+aes(x=hat,y=rs,size=cd)+xlab("hat value")+theme(legend.position="none")+ ylab("studentized residual")
#Model4:log transformation on log(earn+1) and log(height)
Model4<-lm(log(earn)~log(height),data = heights_final)
summary(Model4)
#Model5:square transformation 
Model5<-lm(log(earn)~1/height,data = heights_final)
summary(Model5)
```
I prefer Model3, which regresses $log(earn)$ on certered height. We expect that earning is a data highly right-skewed, so it's reasonable to transform it into log. Centering heights on mean also improve interpretability. 
Among all models fitted above, Model3 is the one with a (little) higher R square, higher F-statistics and smaller residual standard error, even though all models fitted above are of low quality, and all models' residual plots are bad... 

4. Interpret all model coefficients.
Model3 can be written like:

$log(earn)=9.7+0.06height_{center}$

If we have a person of average height, he or she will have a earning of the amount $e^{9.7}$, and one unit higher than the height's mean value makes the person's earning $e^{0.06}$ times higher. The same for a unit lower than the height's mean value.

5. Construct 95% confidence interval for all model coefficients and discuss what they mean.

```{r}
round(confint(Model3,level = 0.95))
```
This result means that:
-we have 95% of confidence that the range [8.22,8.57] will include the intercept coefficient's true value;
-we have 95% of confidence that that the range [0.19,0.28] will include the height_center coefficient's true value. 

### Analysis of mortality rates and various environmental factors

The folder `pollution` contains mortality rates and various environmental factors from 60 U.S. metropolitan areas from McDonald, G.C. and Schwing, R.C. (1973) 'Instabilities of regression estimates relating air pollution to mortality', Technometrics, vol.15, 463-482. 

Variables, in order:

* PREC   Average annual precipitation in inches
* JANT   Average January temperature in degrees F
* JULT   Same for July
* OVR65  % of 1960 SMSA population aged 65 or older
* POPN   Average household size
* EDUC   Median school years completed by those over 22
* HOUS   % of housing units which are sound & with all facilities
* DENS   Population per sq. mile in urbanized areas, 1960
* NONW   % non-white population in urbanized areas, 1960
* WWDRK  % employed in white collar occupations
* POOR   % of families with income < $3000
* HC     Relative hydrocarbon pollution potential
* NOX    Same for nitric oxides
* SO@    Same for sulphur dioxide
* HUMID  Annual average % relative humidity at 1pm
* MORT   Total age-adjusted mortality rate per 100,000

For this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons as inputs. This model is an extreme oversimplification as it combines all sources of mortality and does not adjust for crucial factors such as age and smoking. We use it to illustrate log transformations in regression.

```{r}
gelman_dir   <- "http://www.stat.columbia.edu/~gelman/arm/examples/"
pollution    <- read.dta (paste0(gelman_dir,"pollution/pollution.dta"))
```

1. Create a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.

```{r}
model_polnox<-lm(mort~nox,data = pollution)
summary(model_polnox)
plot(y = pollution$mort,x = pollution$nox)
abline(model_polnox,col="blue")
plot(model_polnox,which=1)
```
The model_polnox we applied here doesn't fit data well, but it seems that the scatters can be well fitted by a specific linear regression model.
The residual plot is of low quality, it tends to have a pattern and the sum of residuals seem not equal zero.

2. Find an appropriate transformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.

Based on our observation on the data's scatter plot, it's obvious that a cluster exists on the left hand side of the plot, so we use a log transformation on $X_{nox}$.
```{r}
model_polnox2<-lm(mort~log(nox),data=pollution)
summary(model_polnox2)
plot(y = pollution$mort,x = log(pollution$nox))
abline(model_polnox2,col="red")
resid(model_polnox2)
plot(model_polnox2,which=1)

```
Using $log(X_{nox})$, the data are much more dispersive. The residual plot has higher quality, with a more balanced distribution above and below zero and less centralized to one side.

3. Interpret the slope coefficient from the model you chose in 2.

The transformed model:

$Y_{mort}=904+15 log(X_{nox})$

When $X_{nox}=1$, we'll have a adjusted mortality rate of 904 per 100,000. 
Holding other variables, 1 unit change in $log(X_{nox})$ will lead to 15 unit change in mortality. 

4. Construct 99% confidence interval for slope coefficient from the model you chose in 2 and interpret them.

```{r}
round(confint(object = model_polnox2,parm = c("log(nox)"),level = 0.99))
```
We have 99% of confidence that the slope coefficient will drop in the range [-2.23,32.90].

5. Now fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformations when
helpful. Plot the fitted regression model and interpret the coefficients.

```{r}
center_so2<-pollution$so2-mean(pollution$so2)
model_3input<-lm(mort~log(nox)+center_so2+log(hc),data = pollution)
summary(model_3input)
ggplot(model_3input)+aes(y=pollution$mort,x=log(pollution$nox)+center_so2+log(pollution$hc))+geom_point(color="orange")+geom_smooth()
```
Based on the oberservation of data's distribution of $nox$, $so2$ and $hc$, we've seen that $nox$ and $hc$ are clustered on one side so we apply log transformation on them. The variable of $so2$ is more dispersive, so we centered $so2$. 

$Y_{mort}=957+56log(X_{nox})+0.26X_{centerso2}-53log(X_{hc})$

If we have $X_{nox}=1$, $X_{hc}=1$ and $X_{centerso2}$ of average level, $Y_{mort}=0$.
Holding other variables, one unit from the so2 average will lead to 0.26 unit difference on mortality; one unit change in $log(X_{nox})$ will lead to 56 units difference on mortality; one unit change in $log(x_{hc})$ will lead to -53 difference on mortality.

6. Cross-validate: fit the model you chose above to the first half of the data and then predict for the second half. (You used all the data to construct the model in 4, so this is not really cross-validation, but it gives a sense of how the steps of cross-validation can be implemented.)

```{r}
dim(pollution)
data1<-pollution[1:30,]
data2<-pollution[31:60,]
center_so2_2<-data1$so2-mean(data1$so2)
model_data1<-lm(mort~log(nox)+center_so2_2+log(hc),data =data1)

prediction<-predict(object = model_data1,newdata = data.frame(nox=data2$nox,center_so2_2=data2$so2-mean(data2$so2),hc=data2$hc),interval= "prediction") #the first column in this result is the fitted value, the second is the lower side of confidence interval and the third is the upper side.

#the difference between the actual values and the predicted values
difference<-prediction[,1]-pollution[31:60,]$mort
plot(difference)
```

### Study of teenage gambling in Britain

```{r,message =FALSE}
data(teengamb)
?teengamb
```

1. Fit a linear regression model with gamble as the response and the other variables as predictors and interpret the coefficients. Make sure you rename and transform the variables to improve the interpretability of your regression model.

```{r}
log_income<-log(teengamb$income)
center_status<-teengamb$status-mean(teengamb$status)
center_verbal<-teengamb$verbal-mean(teengamb$verbal)
log_gamble<-teengamb$gamble
model_gamble<-lm(log(gamble+1)~sex+center_status+log(income)+center_verbal,data =teengamb)
summary(model_gamble)

```
This model_gamble:

$log(Y_{gamble+1})=1.22-1.10X_{sex}+0.02X_{center status}+0.93log(X_{income})-0.25X_{center verbal}$

The difference between woman and man, if we hold the other variables as constant, the $log(Y_{gamble+1})$ of woman is 1.10 more than man. Holding other variables, one unit change of $log(X_{income})$ leads to 0.93 unit difference on $log(Y_{gamble+1})$; one unit from status's average leads to 0.02 unit difference on $log(Y_{gamble+1})$.

2. Create a 95% confidence interval for each of the estimated coefficients and discuss how you would interpret this uncertainty.

```{r}
round(confint(object = model_gamble,level = 0.95))
```
We have 95% of confidence that the range [0.41,2.02] will include the intercept coefficient's true value.
We have 95% of confidence that the range [-1.89,-0.31] will include the sex coefficient's true value.
We have 95% of confidence that the range [-0.003,0.05] will include the centered status coefficient's true value.
We have 95% of confidence that the range [0.46,1.41] will include the log income coefficient' true value.
We have 95% of confidence that the range [-0.47,-0.039] will include the centered verbal coefficient's true value.

3. Predict the amount that a male with average status, income and verbal score would gamble along with an appropriate 95% CI.  Repeat the prediction for a male with maximal values of status, income and verbal score.  Which CI is wider and why is this result expected?

```{r}
prediction_average<-predict(object = model_gamble,newdata = data.frame(sex=0,center_status=mean(teengamb$status)-mean(teengamb$status),income=log(mean(teengamb$income)),center_verbal=mean(teengamb$verbal)-mean(teengamb$verbal)),level = 0.95,interval  = "prediction")
prediction_average

prediction_max<-predict(object = model_gamble,newdata =data.frame(sex=0,center_status=max(teengamb$status)-mean(teengamb$status),income=log(max(teengamb$income)),center_verbal=max(teengamb$verbal)-mean(teengamb$verbal)),level = 0.95,interval = "prediction")
prediction_max
```
The second one with max values is wider. Because if we look at the confidence interval formula:
(for an unknown standard deviation) $(\bar{x}-t^{\star}\frac{s}{\sqrt(n)}???\bar{x}+t^{\star}\frac{s}{\sqrt(n)})$
The second model uses maximal values of variables, so $s$ in the formula will be larger than the one in the first model(with average values). With the other parameters in the formula the same, confidence interval for the second model will be wider.

### School expenditure and test scores from USA in 1994-95

```{r}
data(sat)
?sat
```

1. Fit a model with total sat score as the outcome and expend, ratio and salary as predictors.  Make necessary transformation in order to improve the interpretability of the model.  Interpret each of the coefficient.

```{r}
center_ratio<-(sat$ratio-mean(sat$ratio))/sd(sat$ratio)
center_salary<-(sat$salary-mean(sat$salary))/sd(sat$salary)
model_sat<-lm(log(total)~log(expend)+center_ratio+center_salary,data = sat)
summary(model_sat)
plot(resid(model_sat))
abline(h=0)
```

$log(Y_{total})=6.77+0.06log(X_{expend})+0.01(X_{center ratio})-0.04(X_{center salary})$
Controlling other variable, one unit far from ratio's mean leads to a 0.01 times difference in $log(Y_{total})$, one unit far from salary's mean leads to a -0.04 times difference in $log(Y_{total})$. And if we have a person with average ratio, average salary and $X_{expend}=1$, $log(Y_{total})=6.77$.

2. Construct 98% CI for each coefficient and discuss what you see.

```{r}
round(confint(object = model_sat,level = 0.98))
```
We have 98% of confidence that the range [6.17,7.36] will include the intercept coefficient's true value.
We have 98% of confidence that the range [-0.28,0.40] will include the log(expend) coefficient's true value.
We have 98% of confidence that the range [-0.02,0.05] will include the centered ratio coefficient's true value.
We have 98% of confidence that the range [-0.11,0.02] will include the centered salary coefficient's true value.


3. Now add takers to the model.  Compare the fitted model to the previous model and discuss which of the model seem to explain the outcome better?

```{r}
center_taker<-sat$takers-mean(sat$takers)
model_sat_taker<-lm(log(total)~log(expend)+center_ratio+center_salary+center_taker,data = sat)
summary(model_sat_taker)
plot(resid(model_sat_taker))
abline(h=0)
```
This model is better comapred to the previous one. Normally, if we add a related variable into the regression, the quality of model will improve.
-The R squared is much more higher. 0.81 indicated that the model can explain 81% of the data.
-The center_taker we added is a statistical significant one in the model. 

# Conceptual exercises.

### Special-purpose transformations:

For a study of congressional elections, you would like a measure of the relative amount of money raised by each of the two major-party candidates in each district. Suppose that you know the amount of money raised by each candidate; label these dollar values $D_i$ and $R_i$. You would like to combine these into a single variable that can be included as an input variable into a model predicting vote share for the Democrats.

Discuss the advantages and disadvantages of the following measures:

* The simple difference, $D_i-R_i$
This measurement can show absolute numerical difference, which is easier to interpret, we can just say one unit of the numerical difference between two parties' amount will lead to how many unit of difference. But this doesn't take proportion into account.

* The ratio, $D_i/R_i$
This measurement can show relative proportion difference, which is also easily interpretable. The disadvantage is that $200/100=2/1=2$, which has no information on the actual values of variables. 

* The difference on the logarithmic scale, $log D_i-log R_i$ 
$log D_i-log R_i = log(\frac{D_i}{R_i})$
This measurement on the one hand gives the inforamtion about proportion (so may also have the disadvantage of proportion measurement) on the other hand log transformation can adjust data with problem. However, this process makes the model less interpretable.

* The relative proportion, $D_i/(D_i+R_i)$.
This measurement shows the participation, so the total amount of both is taken into account. It gives us an idea of the influence of the percentage of $D_i$ in total amount on response variable. It's less interpretable. 

### Transformation 

For observed pair of $\mathrm{x}$ and $\mathrm{y}$, we fit a simple regression model 
$$\mathrm{y}=\alpha + \beta \mathrm{x} + \mathrm{\epsilon}$$ 
which results in estimates $\hat{\alpha}=1$, $\hat{\beta}=0.9$, $SE(\hat{\beta})=0.03$, $\hat{\sigma}=2$ and $r=0.3$.

1. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=\mathrm{x}-10$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star}$, $\hat{\beta}^{\star}$, $\hat{\sigma}^{\star}$, and $r^{\star}$.  What happens to these quantities when $\mathrm{x}^{\star}=10\mathrm{x}$ ? When $\mathrm{x}^{\star}=10(\mathrm{x}-1)$?

When $x^{\star}=x-10$, 

$\hat{\alpha}^{\star}=\hat{\alpha}+10\hat{\beta}=10$, 

$\hat{\beta}^{\star}=\hat{\beta}=0.9$, 

$r^{\star}=r=0.3$

$\hat{\sigma}^{\star}=\hat{\sigma}=2$.


When $x^{\star}=10x$, 

$\hat{\alpha}^{\star}=\hat{\alpha}=1$, 

$\hat{\beta}^{\star}=\frac{\hat{\beta}}{10}=0.09$, 

$r^{\star}=r=0.3$

$\hat{\sigma}^{\star}=\hat{\sigma}=2$.


When $x^{\star}=10(x-1)$, 

$\hat{\alpha}^{\star}=\hat{\alpha}+\hat{\beta}=1.9$, 

$\hat{\beta}^{\star}=\frac{\hat{\beta}}{10}=0.09$, 

$r^{\star}=r=0.3$

$\hat{\sigma}^{\star}=\hat{\sigma}=2$.


2. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}= \mathrm{y}+10$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $\hat{\alpha}^{\star\star}$, $\hat{\beta}^{\star\star}$, $\hat{\sigma}^{\star\star}$, and $r^{\star\star}$.  What happens to these quantities when $\mathrm{y}^{\star\star}=5\mathrm{y}$ ? When $\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$?

When $y^{\star\star}= y+10$, 

$\hat{\alpha}^{\star\star}=\hat{\alpha}+10=11$, 

$\hat{\beta}^{\star\star}=\hat{\beta}=0.9$,  

$r^{\star}=r=0.3$

$\hat{\sigma}^{\star\star}=\hat{\sigma}=2$.

When $y^{\star\star}=5y$, 

$\hat{\alpha}^{\star\star}=5\hat{\alpha}=5$, 

$\hat{\beta}^{\star\star}=5\hat{\beta}=4.5$, 

$r^{\star}=r=0.3$

$\hat{\sigma}^{\star\star}=5\hat{\sigma}=10$.

When $y^{\star\star}=5(y+2)$, 

$\hat{\alpha}^{\star\star}=5(\hat{\alpha}+2)=15$, 

$\hat{\beta}^{\star\star}=5\hat{\beta}=4.5$, 

$r^{\star}=r=0.3$

$\hat{\sigma}^{\star\star}=5\hat{\sigma}=10$.


3. In general, how are the results of a simple regression analysis affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?

In transformations of $\mathrm{y}$, y plus or minus a constant number will only change the intercept coefficient; y times a constant number will change the intercept, the slope coefficients and also the residual. 
In transformations of $\mathrm{x}$, x plus or minus a constant number will only change the intercept coefficient; x times a constant number will only change the slope coefficient. 

4. Suppose that the explanatory variable values in a regression are transformed according to the $\mathrm{x}^{\star}=10(\mathrm{x}-1)$ and that $\mathrm{y}$ is regressed on $\mathrm{x}^{\star}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star})$ and $t^{\star}_0= \hat{\beta}^{\star}/SE(\hat{\beta}^{\star})$.

$SE(\hat{\beta}^{\star})=SE(\hat{\beta})=0.03$

$t^{\star}_0=\frac{0.09}{0.03}=3$

5. Now suppose that the response variable scores are transformed according to the formula
$\mathrm{y}^{\star\star}=5(\mathrm{y}+2)$ and that $\mathrm{y}^{\star\star}$ is regressed on $\mathrm{x}$.  Without redoing the regression calculation in detail, find $SE(\hat{\beta}^{\star\star})$ and $t^{\star\star}_0= \hat{\beta}^{\star\star}/SE(\hat{\beta}^{\star\star})$.

$SE(\hat{\beta}^{\star})=5SE(\hat{\beta})=0.15$

$t^{\star}_0=\frac{4.5}{0.15}=30$

6. In general, how are the hypothesis tests and confidence intervals for $\beta$ affected by linear transformations of $\mathrm{y}$ and $\mathrm{x}$?
Hypothesis tests: if we take student test statistics as example,

$t_o=\frac{\hat{\beta}-\mu_0}{SE}$~$t(n-1)$

-Addition and multiplication transformations of $\mathrm{y}$ or $\mathrm{x}$ won't change hypothese tests. 
We take $x^{\star}=x-10$ and $y^{\star}=5y$ as example. 

$x^{\star}=x-10$ don't change $\hat{\beta}$ or $SE(\hat{\beta})$, so $t_0$ doesn't change;

$y^{\star}=5y$, $t_0=\frac{5\hat{\beta}-\mu_0}{5SE(\hat{\beta})}$, so $t_0$ doesn't change.

-Addition of $\mathrm{y}$ or $\mathrm{x}$ won't change confidence interval. But multiplication will.

If $x^{\star}=10x$, CI becomes: $[\frac{1}{10}\hat{\beta}-t_{\alpha/2}\frac{SE}{10},\frac{1}{10}\hat{\beta}+t_{\alpha/2}\frac{SE}{10}]$

If $y^{\star}=5y$, CI becomes:
$[5\hat{\beta}-t_{\alpha/2}5SE,5\hat{\beta}+t_{\alpha/2}5SE]$


		
# Feedback comments etc.

If you have any comments about the homework, or the class, please write your feedback here.  We love to hear your opinions.

