---
title: "Homework 03"
subtitle: "Logistic Regression"
author: "Your name"
date: "September 11, 2018"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,dev="CairoPNG",fig.align = "center", 
                      fig.width = 5.656, fig.height = 4, global.par = TRUE)
#install.packages("pacman",repos="https://cloud.r-project.org")
pacman::p_load("ggplot2","knitr","arm","foreign","car","Cairo","data.table")
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
```



# Data analysis 

### 1992 presidential election

The folder `nes` contains the survey data of presidential preference and income for the 1992 election analyzed in Section 5.1, along with other variables including sex, ethnicity, education, party identification, and political ideology.

```{r, echo=FALSE}
nes5200<-read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/nes/nes5200_processed_voters_realideo.dta")
#saveRDS(nes5200,"nes5200.rds")
#nes5200<-readRDS("nes5200.rds")

nes5200_dt <- data.table(nes5200)
  yr <- 1992
nes5200_dt_s<-nes5200_dt[ year==yr & presvote %in% c("1. democrat","2. republican")& !is.na(income)]
nes5200_dt_s<-nes5200_dt_s[,vote_rep:=1*(presvote=="2. republican")]
nes5200_dt_s$income <- droplevels(nes5200_dt_s$income)
```

1.  Fit a logistic regression predicting support for Bush given all these inputs. Consider how to include these as regression predictors and also consider possible interactions.

We first need to change data's input as integer (to eliminate unusful strings), and create a data only with selected variables. 
The logistic model fitted here is applying all the inputs with interactions of gender and race, income and educ1.
```{r}
#Change data to numeric variables
nes5200_dt_s$vote_rep <- as.integer(nes5200_dt_s$vote_rep)
nes5200_dt_s$gender <- as.integer(nes5200_dt_s$gender)
nes5200_dt_s$race <- as.integer(nes5200_dt_s$race)
nes5200_dt_s$educ1 <- as.integer(nes5200_dt_s$educ1)
nes5200_dt_s$income <- as.integer(nes5200_dt_s$income)
nes5200_dt_s$partyid7 <- as.integer(nes5200_dt_s$partyid7)
nes5200_dt_s$ideo_feel <- as.integer(nes5200_dt_s$ideo_feel)

#Create a data with selected columns 
library(dplyr)
nes5200_dt_s %>% 
  select(vote_rep,gender,race,educ1,income,partyid7,ideo_feel) -> data_nes5200
new_nes5200 = na.omit(data_nes5200)
#GLM model
model_nes5200 <- glm(vote_rep~gender*race+income*educ1+partyid7+ideo_feel,family = binomial,data=new_nes5200)
summary(model_nes5200)

#Model regression estimates plot
coefplot(model_nes5200)
#Binned residual plot
binnedplot(fitted(model_nes5200),resid(model_nes5200,type = "response"))

```

2. Evaluate and compare the different models you have fit. Consider coefficient estimates and standard errors, residual plots, and deviances.

We try to fit models with various combinations of inputs.
```{r}
#model with only the interaction of income and educ1
model_nes5200_2 <- glm(vote_rep~gender+race+income*educ1+partyid7+ideo_feel,family = binomial,data=new_nes5200)
summary(model_nes5200_2)

#model without interaction
model_nes5200_3 <- glm(vote_rep~gender+race+income+educ1+partyid7+ideo_feel,family = binomial,data=new_nes5200)
summary(model_nes5200_3)

#looking at the plots
par(mfrow=c(2,2))
coefplot(model_nes5200_2)
binnedplot(fitted(model_nes5200),resid(model_nes5200_2,type = "response"))
coefplot(model_nes5200_3)
binnedplot(fitted(model_nes5200),resid(model_nes5200_3,type = "response"))

```
I will choose the model_nes5200:
-with the regression estimates plots and the summaries, model_nes5200 has more significant variables, which is four. And the other two models only have three significant variables;
-the residual deviance in model_nes5200 is slightly lower than the other two;
-the AICs shown in summary don't have large difference in these three models, so we don't take this small difference as a definitive selection criterion; also same for the binned residual plots.

3. For your chosen model, discuss and compare the importance of each input variable in the prediction.

We want to interpret all significant variables in this model:
-Intercept: if a person has categories with zero gender, race, income, educ1, partyid7, and zero ideo_feel, he will have log odds of $-10.68$ to vote for Bush.
-Race: holding all other variables, if race increase by 1 category, the expected value of the voter's log odds for Bush will decrease by 1.59 units.
-Partyid7: holding all other variables, if partyid7 increase by 1 category, the expected value of the voter's log odds for Bush will decrease by 0.48 unit.
-ideo_feel: holding all other variables, if ideo_feel increase by 1 unit, the expected value of the voter's log odds for Bush will increase by 0.09 unit.
-gender:race: holding all other variables, if the interaction of gender and race increase by 1 category, the expected value of the voter's log odds for Bush will increase by 0.64 unit.

```{r}
summary(model_nes5200)
```

### Graphing logistic regressions: 

the well-switching data described in Section 5.4 of the Gelman and Hill are in the folder `arsenic`.  

```{r, echo=FALSE}
wells <- read.table("http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat", header=TRUE)
wells_dt <- data.table(wells)
```

1. Fit a logistic regression for the probability of switching using log (distance to nearest safe well) as a predictor.
```{r}
model_wells <- glm(switch~log(dist),family =binomial(link = "logit"),data = wells_dt)
summary(model_wells)
```

2. Make a graph similar to Figure 5.9 of the Gelman and Hill displaying Pr(switch) as a function of distance to nearest safe well, along with the data.
```{r}
jitter.binary <- function(a, jitt=.05){
ifelse (a==0, runif (length(a), 0, jitt), runif (length(a), 1-jitt, 1))
}

switch.jitter <- jitter.binary (wells_dt$switch)
plot (log(wells_dt$dist), switch.jitter)
curve (invlogit (coef(model_wells)[1] + coef(model_wells)[2]*x), add=TRUE)
  
```

3. Make a residual plot and binned residual plot as in Figure 5.13.
```{r}
plot(fitted(model_wells),resid(model_wells,type="response"))
abline(h=0,lty=3)

binnedplot(fitted(model_wells),resid(model_wells,type="response"))
```

4. Compute the error rate of the fitted model and compare to the error rate of the null model.

```{r}
predicted <- fitted(model_wells)
error.rate <- mean ((predicted>0.5 & wells_dt$switch==0) | (predicted<.5 & wells_dt$switch==1))
error.rate.null <- min(mean(wells_dt$switch),1-mean(wells_dt$switch))
error.rate
error.rate.null
```
This model's influence is not significant, with two closed error rates.

5. Create indicator variables corresponding to `dist < 100`, `100 =< dist < 200`, and `dist > 200`. Fit a logistic regression for Pr(switch) using these indicators. With this new model, repeat the computations and graphs for part (1) of this exercise.

```{r}
#new model with categorial variables
dist_wells <- wells_dt$dist
dist_wells[dist_wells<100] <- 1
dist_wells[dist_wells>=100 & dist_wells<200] <- 2
dist_wells[dist_wells>=200] <- 3
model_pr <- glm(switch~dist_wells,family=binomial(link="logit"),data = wells_dt)
summary(model_pr)

#graph with fitted model and jitter points
jitter.binary <- function(a, jitt=.05){
ifelse (a==0, runif (length(a), 0, jitt), runif (length(a), 1-jitt, 1))
}

switch.jitter <- jitter.binary (wells_dt$switch)
plot (dist_wells, switch.jitter)
curve (invlogit (coef(model_pr)[1] + coef(model_pr)[2]*x), add=TRUE)

#residual plots
plot(fitted(model_pr),resid(model_wells,type="response"))
abline(h=0,lty=3)

binnedplot(fitted(model_pr),resid(model_pr,type="response"))

#error rate
predicted <- fitted(model_pr)
error.rate <- mean ((predicted>0.5 & wells_dt$switch==0) | (predicted<.5 & wells_dt$switch==1))
error.rate.null <- min(mean(wells_dt$switch),1-mean(wells_dt$switch))
error.rate
error.rate.null
```
This new model has a larger influence on error rates. 

### Model building and comparison: 
continue with the well-switching data described in the previous exercise.

1. Fit a logistic regression for the probability of switching using, as predictors, distance, `log(arsenic)`, and their interaction. Interpret the estimated coefficients and their standard errors.

```{r}
model_switch <- glm(switch~dist+log(arsenic)+dist*log(arsenic),family = binomial(link="logit"),data = wells_dt)
summary(model_switch)
```
-Intercept: $logit^{-1}(0.49)$ is the estimated probability of switching, if dist=0, arsenic=1 and dist:log(arsenic)=0.
-Dist: if we hold other variables, one unit increase in distance will lead to a decrease of 0.008 unit on log odds of switch.
-log(arsenic): if we hold other variables, one unit increase in log(arsenic) will lead to an increase of 0.98 unit on log odds of switch.
-Dist:Log(arsenic): if we hold other variables, one unit increase in dist*log(arsenic) will lead to a decrease of 0.0023 on log odds of switch, but this variable is not significant.

Using standard error, we can have a glance of coefficients' confidence intervals:
```{r}
coefplot(model_switch)
```

2. Make graphs as in Figure 5.12 to show the relation between probability of switching, distance, and arsenic level.

```{r}
plot(wells_dt$dist,switch.jitter,xlim=c(0,max(wells_dt$dist)))
curve(invlogit(cbind(1,x,0.5,0.5*x) %*% coef(model_switch)),add=TRUE)
curve(invlogit(cbind(1,x,-0.3,-0.3*x) %*% coef(model_switch)),add=TRUE)

plot(log(wells_dt$arsenic),switch.jitter,xlim=c(0,max(log(wells_dt$arsenic))))
curve(invlogit(cbind(1,0,x,0) %*% coef(model_switch)),add=TRUE)
curve(invlogit(cbind(1,50,x,50*x) %*% coef(model_switch)),add=TRUE)
```

3. Following the procedure described in Section 5.7, compute the average predictive differences corresponding to:
i. A comparison of dist = 0 to dist = 100, with arsenic held constant. 
ii. A comparison of dist = 100 to dist = 200, with arsenic held constant.
iii. A comparison of arsenic = 0.5 to arsenic = 1.0, with dist held constant. 
iv. A comparison of arsenic = 1.0 to arsenic = 2.0, with dist held constant.
Discuss these results.

```{r}
#dist = 0 to dist = 100
b <- coef (model_switch)
hi <- 100
lo <- 0
delta <- invlogit (b[1] + b[2]*hi + b[3]*log(wells_dt$arsenic) + b[4]*(hi*log(wells_dt$arsenic))) -
  invlogit (b[1] + b[2]*lo + b[3]*log(wells_dt$arsenic) + b[4]*(lo*log(wells_dt$arsenic)))
print (mean(delta))
#dist = 100 to dist = 200
hi <- 200
lo <- 100
delta <- invlogit (b[1] + b[2]*hi + b[3]*log(wells_dt$arsenic) + b[4]*(hi*log(wells_dt$arsenic))) -
  invlogit (b[1] + b[2]*lo + b[3]*log(wells_dt$arsenic) + b[4]*(lo*log(wells_dt$arsenic)))
print (mean(delta))
#arsenic = 0.5 to arsenic = 1.0
hi <- 1.0
lo <- 0.5
delta <- invlogit (b[1] + b[2]*wells_dt$dist + b[3]*log(hi) + b[4]*(wells_dt$dist*log(hi))) -
  invlogit (b[1] + b[2]*wells_dt$dist + b[3]*log(lo) + b[4]*(wells_dt$dist*log(lo)))
print (mean(delta))
#arsenic = 1.0 to arsenic = 2.0
hi <- 2.0
lo <- 1.0
delta <- invlogit (b[1] + b[2]*wells_dt$dist + b[3]*log(hi) + b[4]*(wells_dt$dist*log(hi))) -
  invlogit (b[1] + b[2]*wells_dt$dist + b[3]*log(lo) + b[4]*(wells_dt$dist*log(lo)))
print (mean(delta))
```
The first result is ???0.211, implying that, on average in the data, households that are 100 meters from the nearest safe well are 21% less likely to switch, compared to househoulds that are right next to the nearest safe well, at the same arsenic levels.
The second result is ???0.209, implying that, on average in the data, households that are 200 meters from the nearest safe well are 21% less likely to switch, compared to househoulds that are 100 meters from the nearest safe well, at the same arsenic levels.
The third result is 0.146, implying that, on average in the data, respondents with wells that have a log arsenic level of 1.0 are 14% more likely to switch, respondents with wells that have a log arsenic level of 0.5.
The forth result is 0.140, implying that, on average in the data, respondents with wells that have a log arsenic level of 2.0 are 14% more likely to switch, respondents with wells that have a log arsenic level of 1.0.

### Building a logistic regression model: 
the folder rodents contains data on rodents in a sample of New York City apartments.

Please read for the data details.
http://www.stat.columbia.edu/~gelman/arm/examples/rodents/rodents.doc

```{r read_rodent_data, echo=FALSE}
apt.subset.data <- read.table ("http://www.stat.columbia.edu/~gelman/arm/examples/rodents/apt.subset.dat", header=TRUE)
apt_dt <- data.table(apt.subset.data)
setnames(apt_dt, colnames(apt_dt),c("y","defects","poor","race","floor","dist","bldg")
)
invisible(apt_dt[,asian := race==5 | race==6 | race==7])
invisible(apt_dt[,black := race==2])
invisible(apt_dt[,hisp  := race==3 | race==4])

```

1. Build a logistic regression model to predict the presence of rodents (the variable y in the dataset) given indicators for the ethnic groups (race). Combine categories as appropriate. Discuss the estimated coefficients in the model.

```{r}
model_race <- glm(y~asian+black+hisp,family = binomial(link="logit"),data = apt_dt)
summary(model_race)
```
-Intercept:when a person is not an asian, black or hisp, log odds of rodents' presence is -2.15;
-Asian:if a person is asian (so black=0 and hisp=0), log odds of rodents' presence increases 0.55 unit;
-Black:if a person is black, log odds of rodents' presence increases 1.53 units;
-Hisp:if a person is hisp, log odds of rodents' presence increases 1.70 units.

2. Add to your model some other potentially relevant predictors describing the apartment, building, and community district. Build your model using the general principles explained in Section 4.6 of the Gelman and Hill. Discuss the coefficients for the ethnicity indicators in your model.

```{r}
model_rodents <- glm(y~race+dist+defects+poor,data = apt_dt)
summary(model_rodents)
```
-Intercept:if race, dist, defects and poor all equal zero, the log odds of y will be 0.06;
-Race:holding other variables, one unit increase in race category will lead to 0.031 unit increase in log odss of y;
-Dist:holding other variables, one unit increase in dist category will lead to 0.002 unit decrease in log odds of y;
-Defects:holding other variables, one unit increase in defects will lead to 0.091 unit increase in log odds of y;
-Poor:holding other variables, one unit increase in poor will lead to 0.03 unit increase in log odds of y.

# Conceptual exercises.

### Shape of the inverse logit curve

Without using a computer, sketch the following logistic regression lines:
(Photos attached)

1. $Pr(y = 1) = logit^{-1}(x)$
2. $Pr(y = 1) = logit^{-1}(2 + x)$
3. $Pr(y = 1) = logit^{-1}(2x)$
4. $Pr(y = 1) = logit^{-1}(2 + 2x)$
5. $Pr(y = 1) = logit^{-1}(-2x)$


### 
In a class of 50 students, a logistic regression is performed of course grade (pass or fail) on midterm exam score (continuous values with mean 60 and standard deviation 15). The fitted model is $Pr(pass) = logit^{-1}(-24+0.4x)$.

1. Graph the fitted model. Also on this graph put a scatterplot of hypothetical data consistent with the information given.

```{r}
score <- rnorm(50, mean=60, sd = 15)
prob_pass <- invlogit(-24 + 0.4*score)
pass <- ifelse(prob_pass>.5,1,0)
ggplot(data.frame(x=c(0, 8)), aes(x)) +
stat_function(fun=function(x)+invlogit(logit(0.27)+(logit(0.88)-logit(0.27))/6 * x))+labs(x="earnings (in $10,000)", y="probability")
```

2. Suppose the midterm scores were transformed to have a mean of 0 and standard deviation of 1. What would be the equation of the logistic regression using these transformed scores as a predictor?

The transformation is $score_{center} = (score-mean)/sd$. So the model is written:

$Pr(pass)=logit^{-1}(-24+0.4*(\frac{score-60}{15}))=logit^{-1}(6x)$

3. Create a new predictor that is pure noise (for example, in R you can create `newpred <- rnorm (n,0,1)`). Add it to your model. How much does the deviance decrease?

```{r}
newpred <- rnorm(n=50,0,1)
de_1 <- deviance(glm(prob_pass~score,family = binomial(link="logit")))
de_2 <- deviance(glm(prob_pass~score+newpred,family = binomial(link="logit")))
de_1>=de_2
```
With $de_1>=de_2$ = FALSE, the noise added reduce the model's deviance.

### Logistic regression

You are interested in how well the combined earnings of the parents in a child's family predicts high school graduation. You are told that the probability a child graduates from high school is 27% for children whose parents earn no income and is 88% for children whose parents earn 60,000. Determine the logistic regression model that is consistent with this information. (For simplicity you may want to assume that income is measured in units of 10,000).

$Pr(graduation hs) = logit^{-1}( logit(0.27) + logit(0.88) * parents earning) $

### Latent-data formulation of the logistic model: 
take the model $Pr(y = 1) = logit^{-1}(1 + 2x_1 + 3x_2)$ and consider a person for whom $x_1 = 1$ and $x_2 = 0.5$. Sketch the distribution of the latent data for this person. Figure out the probability that $y=1$ for the person and shade the corresponding area on your graph.


### Limitations of logistic regression: 

consider a dataset with $n = 20$ points, a single predictor x that takes on the values $1, \dots , 20$, and binary data $y$. Construct data values $y_{1}, \dots, y_{20}$ that are inconsistent with any logistic regression on $x$. Fit a logistic regression to these data, plot the data and fitted curve, and explain why you can say that the model does not fit the data.

```{r}
x <- c(1:20)
y <- rep(1,20)
model_y <- glm(y~x,family = binomial(link = "logit"))
ggplot(data.frame(x,y), aes(x=x, y = y)) +
  geom_point(color="blue") +
  stat_function(fun=function(x) invlogit(coef(model_y)[1] + coef(model_y)[2] * x)) +
  labs(x="x", y="y") 
```
In this model, the model curve doesn't fit the scatters at all. 

### Identifiability: 

the folder nes has data from the National Election Studies that were used in Section 5.1 of the Gelman and Hill to model vote preferences given income. When we try to fit a similar model using ethnicity as a predictor, we run into a problem. Here are fits from 1960, 1964, 1968, and 1972:

```{r, echo=FALSE}
nes5200_dt_d<-nes5200_dt[ presvote %in% c("1. democrat","2. republican")& !is.na(income)]
nes5200_dt_d<-nes5200_dt_d[,vote_rep:=1*(presvote=="2. republican")]
nes5200_dt_d$income <- droplevels(nes5200_dt_d$income)

nes5200_dt_d$income <- as.integer(nes5200_dt_d$income)
display(glm(vote_rep ~ female + black + income, data=nes5200_dt_d, family=binomial(link="logit"), subset=(year==1960)))
display(glm(vote_rep ~ female + black + income, data=nes5200_dt_d, family=binomial(link="logit"), subset=(year==1964)))
display(glm(vote_rep ~ female + black + income, data=nes5200_dt_d, family=binomial(link="logit"), subset=(year==1968)))
display(glm(vote_rep ~ female + black + income, data=nes5200_dt_d, family=binomial(link="logit"), subset=(year==1972)))

```

What happened with the coefficient of black in 1964? Take a look at the data and figure out where this extreme estimate came from. What can be done to fit the model in 1964?


# Feedback comments etc.

If you have any comments about the homework, or the class, please write your feedback here.  We love to hear your opinions.

